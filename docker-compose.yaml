services:
  llm:
    build: .
    environment:
      - MODEL=7B/wizardlm-7b-v1.0-uncensored.ggmlv3.q4_1.bin
      - LAYERS=10000
    command: /llama/start.sh
    volumes:
      - type: bind
        source: ./models # set "./models" to the path where you have models saved on your docker host
        target: /models
        read_only: true # mount this folder as read only inside the llm container. llama should not edit your models.
    networks:
      - ai
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  personality:
    build: personality
    environment:
      - PORT=9000
    ports:
      - 9000:9000
    networks:
      - ai

networks:
  ai:
    driver: bridge