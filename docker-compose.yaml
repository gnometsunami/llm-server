services:
  llm:
    build: llm
    environment:
      - MODEL=${MODEL}
      - GPU_LAYERS=${GPU_LAYERS}
      - CUDA_VERSION=${CUDA_VERSION}
      - LLAMA_VERSION=${LLAMA_VERSION}
    command: /llama/start.sh
    volumes:
      - type: bind
        source: ${MODEL_DIR}
        target: /models
        read_only: true
    networks:
      - ai
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  personality:
    build: personality
    environment:
      - PORT=9000
    ports:
      - ${API_PORT}:9000
    networks:
      - ai

networks:
  ai:
    driver: bridge